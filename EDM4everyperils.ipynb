{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n1 ) First Create a BLANK EDM in RISK LINK and import a sample MRI file according to the peril\\n2) Give the name of sample EDM in datbase ( tHIS  L)\\n3)Run the script\\n\\n\\n'"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "1 ) First Create a BLANK EDM in RISK LINK and import a sample MRI file according to the peril\n",
    "2) Give the name of sample EDM in datbase ( )\n",
    "3)Run the script\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV file using Polars\n",
    "df = pl.read_csv(\"input\\THFL_SAMPLE_6.csv\", separator='\\t') # give the location file path here\n",
    "perilno=int(input(\" Enter the peril number you are going to run \"))\n",
    "currency=\"USD\"\n",
    "undate = \"9999-12-31 00:00:00\"\n",
    "Idate = \"12/3/2024 12:00:00 AM\"\n",
    "Edate = \"12/2/2025 12:00:00 AM\"\n",
    "sample_database_name = input (\"enter the name of sample database\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_columns_to_unpivot(perilno):\n",
    "    if perilno == 1:\n",
    "        return [\"EQCV1VAL\", \"EQCV2VAL\", \"EQCV3VAL\"]\n",
    "    elif perilno == 2:\n",
    "        return [\"WSCV1VAL\", \"WSCV2VAL\", \"WSCV3VAL\"]    \n",
    "    elif perilno == 3:\n",
    "        return [\"TOCV1VAL\", \"TOCV2VAL\", \"TOCV3VAL\"]\n",
    "    elif perilno==4:\n",
    "        return [\"FLCV1VAL\", \"FLCV2VAL\", \"FLCV3VAL\"]\n",
    "    elif perilno==5:\n",
    "        return[\"FRCV1VAL\", \"FRCV2VAL\", \"FRCV3VAL\"]\n",
    "    elif perilno==6:\n",
    "        return[\"TRCV1VAL\", \"TRCV2VAL\", \"TRCV3VAL\"]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported perilno value\")\n",
    "\n",
    "\n",
    "\n",
    "# Get columns to unpivot based on perilno\n",
    "columns_to_unpivot = get_columns_to_unpivot(perilno)\n",
    "\n",
    "# Define id_vars\n",
    "id_vars = [col for col in df.columns if col not in columns_to_unpivot]\n",
    "\n",
    "# Unpivot the DataFrame\n",
    "unpivoted_df = df.unpivot(\n",
    "    index=id_vars,\n",
    "    on=columns_to_unpivot,\n",
    "    variable_name=\"losstype\",\n",
    "    value_name=\"Value\"\n",
    ")\n",
    "\n",
    "# Map losstype to corresponding values\n",
    "columns_to_unpivot_mapping = {\n",
    "    \"EQCV1VAL\": 1,\n",
    "    \"EQCV2VAL\": 2,\n",
    "    \"EQCV3VAL\": 3,\n",
    "    \"WSCV1VAL\": 1,\n",
    "    \"WSCV2VAL\": 2,\n",
    "    \"WSCV3VAL\": 3,\n",
    "    \"TOCV1VAL\": 1,\n",
    "    \"TOCV2VAL\": 2,\n",
    "    \"TOCV3VAL\": 3,\n",
    "    \"FLCV1VAL\": 1,\n",
    "    \"FLCV2VAL\": 2,\n",
    "    \"FLCV3VAL\": 3,\n",
    "    \"FRCV1VAL\": 1,\n",
    "    \"FRCV2VAL\": 2,\n",
    "    \"FRCV3VAL\": 3,\n",
    "    \"TRCV1VAL\": 1,\n",
    "    \"TRCV2VAL\": 2,\n",
    "    \"TRCV3VAL\": 3\n",
    "}\n",
    "\n",
    "unpivoted_df = unpivoted_df.with_columns([\n",
    "    pl.col(\"losstype\").map_elements(lambda x: columns_to_unpivot_mapping.get(x, None), return_dtype=pl.Int32).alias(\"losstype\")\n",
    "])\n",
    "\n",
    "# Add COVGMODE column\n",
    "unpivoted_df = unpivoted_df.with_columns([\n",
    "    pl.when(pl.col(\"losstype\") == 2).then(3).otherwise(0).alias(\"COVGMODE\")\n",
    "])\n",
    "\n",
    "# Define the mapping from losstype to labelid\n",
    "losstype_to_labelid = {\n",
    "    1: 7,\n",
    "    2: 8,\n",
    "    3: 9\n",
    "}\n",
    "\n",
    "# Add the labelid column based on the losstype column\n",
    "unpivoted_df = unpivoted_df.with_columns([\n",
    "    pl.col(\"losstype\").map_elements(lambda x: losstype_to_labelid.get(x, None), return_dtype=pl.Int32).alias(\"labelid\")\n",
    "])\n",
    "\n",
    "# Sort the DataFrame by LOCNUM\n",
    "unpivoted_df = unpivoted_df.sort(\"LOCNUM\")\n",
    "\n",
    "# Add the LOCCVGID column\n",
    "unpivoted_df = unpivoted_df.with_columns([\n",
    "    pl.arange(1, unpivoted_df.height + 1).alias(\"LOCCVGID\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (18, 19)\n",
      "┌─────────┬───────────┬──────────┬────────┬───┬──────────┬──────────┬─────────┬──────────┐\n",
      "│ Geoid   ┆ LONGITUDE ┆ LATITUDE ┆ LOCNUM ┆ … ┆ Value    ┆ COVGMODE ┆ labelid ┆ LOCCVGID │\n",
      "│ ---     ┆ ---       ┆ ---      ┆ ---    ┆   ┆ ---      ┆ ---      ┆ ---     ┆ ---      │\n",
      "│ i64     ┆ f64       ┆ f64      ┆ i64    ┆   ┆ i64      ┆ i32      ┆ i32     ┆ i64      │\n",
      "╞═════════╪═══════════╪══════════╪════════╪═══╪══════════╪══════════╪═════════╪══════════╡\n",
      "│ 1970864 ┆ 4.35      ┆ 50.85    ┆ 4      ┆ … ┆ 10000000 ┆ 0        ┆ 7       ┆ 1        │\n",
      "│ 1970864 ┆ 4.35      ┆ 50.85    ┆ 4      ┆ … ┆ 10000000 ┆ 3        ┆ 8       ┆ 2        │\n",
      "│ 1970864 ┆ 4.35      ┆ 50.85    ┆ 4      ┆ … ┆ 10000000 ┆ 0        ┆ 9       ┆ 3        │\n",
      "│ 1970865 ┆ 3.72      ┆ 51.05    ┆ 5      ┆ … ┆ 10000000 ┆ 0        ┆ 7       ┆ 4        │\n",
      "│ 1970865 ┆ 3.72      ┆ 51.05    ┆ 5      ┆ … ┆ 10000000 ┆ 3        ┆ 8       ┆ 5        │\n",
      "│ …       ┆ …         ┆ …        ┆ …      ┆ … ┆ …        ┆ …        ┆ …       ┆ …        │\n",
      "│ 1970984 ┆ 4.27      ┆ 51.21    ┆ 8      ┆ … ┆ 10000000 ┆ 3        ┆ 8       ┆ 14       │\n",
      "│ 1970984 ┆ 4.27      ┆ 51.21    ┆ 8      ┆ … ┆ 10000000 ┆ 0        ┆ 9       ┆ 15       │\n",
      "│ 1970994 ┆ 3.59      ┆ 50.82    ┆ 9      ┆ … ┆ 10000000 ┆ 0        ┆ 7       ┆ 16       │\n",
      "│ 1970994 ┆ 3.59      ┆ 50.82    ┆ 9      ┆ … ┆ 10000000 ┆ 3        ┆ 8       ┆ 17       │\n",
      "│ 1970994 ┆ 3.59      ┆ 50.82    ┆ 9      ┆ … ┆ 10000000 ┆ 0        ┆ 9       ┆ 18       │\n",
      "└─────────┴───────────┴──────────┴────────┴───┴──────────┴──────────┴─────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "print(unpivoted_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in the dataset: 6\n",
      "Number of splits required: 1\n",
      "Copying database: RMS_EDM_THFL to RMS_EDM_THFL_1\n",
      "Database 'RMS_EDM_THFL_1' dropped successfully if it existed.\n",
      "Database 'RMS_EDM_THFL_1' created successfully.\n",
      "Created Databases: ['RMS_EDM_THFL_1']\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "# Global list to store newly created database names\n",
    "new_databases = []\n",
    "\n",
    "# Function to execute a SQL command using sqlcmd\n",
    "def execute_sql_command(server, sql_query):\n",
    "    try:\n",
    "        subprocess.run(f\"sqlcmd -S {server} -Q \\\"{sql_query}\\\"\", check=True, shell=True)\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"SQL command failed: {e}\")\n",
    "        raise\n",
    "\n",
    "# Function to drop a database if it exists and has a specific extension\n",
    "def drop_database_if_exists(server, database_name):\n",
    "    try:\n",
    "        if \"_\" in database_name:\n",
    "            drop_query = f\"IF EXISTS (SELECT name FROM sys.databases WHERE name = '{database_name}') DROP DATABASE [{database_name}]\"\n",
    "            execute_sql_command(server, drop_query)\n",
    "            print(f\"Database '{database_name}' dropped successfully if it existed.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error dropping database {database_name}: {e}\")\n",
    "\n",
    "# Function to copy a database\n",
    "def copy_database(server, original_database_name, new_database_name, split_number):\n",
    "    \"\"\"\n",
    "    Copies a database on the specified SQL Server instance by performing a backup and restore operation.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"Copying database: {original_database_name} to {new_database_name}\")\n",
    "\n",
    "        # Drop the new database if it already exists and has an extension\n",
    "        drop_database_if_exists(server, new_database_name)\n",
    "\n",
    "        # Define paths for backup and new database files\n",
    "        backup_file = f\"D:\\\\Program Files\\\\Microsoft SQL Server\\\\MSSQLSERVER\\\\MSSQL13.MSSQLSERVER\\\\MSSQL\\\\Backup\\\\{original_database_name}_{split_number}.bak\"\n",
    "        data_file = f\"D:\\\\Program Files\\\\Microsoft SQL Server\\\\MSSQLSERVER\\MSSQL13.MSSQLSERVER\\MSSQL\\\\DATA{new_database_name}.mdf\"\n",
    "        log_file = f\"D:\\\\Program Files\\\\Microsoft SQL Server\\\\MSSQLSERVER\\MSSQL13.MSSQLSERVER\\MSSQL\\\\DATA{new_database_name}_log.ldf\"\n",
    "\n",
    "        # Backup the original database\n",
    "        backup_query = f\"BACKUP DATABASE [{original_database_name}] TO DISK = '{backup_file}'\"\n",
    "        execute_sql_command(server, backup_query)\n",
    "\n",
    "        # Restore the database with a new name\n",
    "        restore_query = (\n",
    "            f\"RESTORE DATABASE [{new_database_name}] FROM DISK = '{backup_file}' \"\n",
    "            f\"WITH MOVE '{original_database_name}' TO '{data_file}', \"\n",
    "            f\"MOVE '{original_database_name}_log' TO '{log_file}'\"\n",
    "        )\n",
    "        execute_sql_command(server, restore_query)\n",
    "\n",
    "        print(f\"Database '{new_database_name}' created successfully.\")\n",
    "        new_databases.append(new_database_name)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during database copy operation:\", e)\n",
    "        \n",
    "# Function to count rows in the DataFrame\n",
    "def count_rows_in_dataframe(df):\n",
    "    return len(df)\n",
    "\n",
    "# Function to split and create databases\n",
    "def split_and_create_databases(df, server, original_database_name, locations_per_split):\n",
    "    \"\"\"\n",
    "    Creates multiple database copies based on the number of rows in the DataFrame and the user-defined split size.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        total_rows = count_rows_in_dataframe(df)\n",
    "        print(f\"Total rows in the dataset: {total_rows}\")\n",
    "\n",
    "        # Determine the number of splits required\n",
    "        num_splits = (total_rows + locations_per_split - 1) // locations_per_split\n",
    "        print(f\"Number of splits required: {num_splits}\")\n",
    "\n",
    "        # Create the necessary database copies\n",
    "        for i in range(1, num_splits + 1):\n",
    "            new_database_name = f\"{original_database_name}_{i}\"\n",
    "            copy_database(server, original_database_name, new_database_name, i)\n",
    "\n",
    "        return new_databases\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred while splitting and creating databases:\", e)\n",
    "        return []\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # User inputs and initial setup\n",
    "    try:\n",
    "        server = \"localhost\"\n",
    "        database_name = sample_database_name\n",
    "        locations_per_split = int(input(\"Enter the number of locations wanted in one split: \"))\n",
    "        # Create database copies\n",
    "        created_databases = split_and_create_databases(df, server, database_name, locations_per_split)\n",
    "\n",
    "        # Print the results\n",
    "        print(\"Created Databases:\", created_databases)\n",
    "\n",
    "    except ValueError:\n",
    "        print(\"Invalid input. Please enter numeric values for the split size.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "## adding old codes here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating database: RMS_EDM_THFL_1\n",
      "All rows deleted from Address table in database RMS_EDM_THFL_1.\n",
      "Data population completed in Address table.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import polars as pl\n",
    "\n",
    "# Class to manage SQL connection\n",
    "class SQLConnection:\n",
    "    def __init__(self, server, database):\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def open(self):\n",
    "        # Open a persistent connection\n",
    "        self.connection = pyodbc.connect(\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={self.server};\"\n",
    "            f\"DATABASE={self.database};\"\n",
    "            \"Trusted_Connection=yes;\"\n",
    "        )\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def execute(self, sql_command):\n",
    "        try:\n",
    "            #print(f\"Executing SQL Command: {sql_command}\")\n",
    "            self.cursor.execute(sql_command)\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing command: {sql_command}\\nException: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "\n",
    "# Function to get table columns\n",
    "def get_table_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to get foreign key columns\n",
    "def get_foreign_key_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"\"\"\n",
    "        SELECT COLUMN_NAME \n",
    "        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "        WHERE TABLE_NAME = '{table_name}' AND CONSTRAINT_NAME IN (\n",
    "            SELECT CONSTRAINT_NAME \n",
    "            FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching foreign key columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Server details\n",
    "server = \"localhost\"\n",
    "\n",
    "# Define table mappings (DataFrame column to database table columns)\n",
    "table_mappings = {\n",
    "    \"Address\": {\n",
    "        \"CountryScheme\": \"CNTRYSCHEME\", \"CountryCode\": \"CNTRYCODE\",\n",
    "        \"CountryRMSCode\": \"CNTRYCODE\", \"Latitude\": \"LATITUDE\", \"Longitude\": \"LONGITUDE\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Define behavior for unspecified columns\n",
    "unspecified_column_behavior = {\n",
    "    \"Address\": {\n",
    "        \"default\": \"' '\",  # Default value for unspecified columns\n",
    "        \"null_columns\": ['ParcelNumber', 'Zone3', 'GeoProductVersion', 'GeoDateTime'],\n",
    "        \"blank_columns\": [],\n",
    "        \"zero_columns\": ['AddressTypeID', 'CountryGeoID', 'Admin1GeoID', 'Admin2GeoID', 'Admin3GeoID', 'CityGeoID', 'PostalCodeGeoID', 'AreaID', 'Zone1GeoID', 'GeoResolutionCode', 'GeoResolutionConfidence', 'GeoAccuracyBuffer', 'GeoDataSourceID', 'GeoDataSourceVersionID', 'Admin4GeoID', 'Admin5GeoID', 'LocationCodeGeoID', 'Zone2GeoID', 'Zone3GeoID', 'Zone4GeoID', 'Zone5GeoID']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Counter for AddressID\n",
    "address_id_counter = 4\n",
    "\n",
    "# Assume df and created_databases are defined elsewhere\n",
    "# Split the DataFrame into chunks\n",
    "chunks = [df[i:i + locations_per_split] for i in range(0, len(df), locations_per_split)]\n",
    "\n",
    "# Open the SQL connection once\n",
    "sql_conn = SQLConnection(server, created_databases[0])\n",
    "sql_conn.open()\n",
    "\n",
    "# Populate each chunk into the corresponding database\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i < len(created_databases):\n",
    "        database = created_databases[i]\n",
    "        print(f\"Populating database: {database}\")\n",
    "\n",
    "        # Switch database if necessary\n",
    "        if sql_conn.database != database:\n",
    "            sql_conn.close()\n",
    "            sql_conn = SQLConnection(server, database)\n",
    "            sql_conn.open()\n",
    "\n",
    "        # Delete existing rows from the Address table\n",
    "        sql_conn.execute(\"DELETE FROM Address\")\n",
    "        print(f\"All rows deleted from Address table in database {database}.\")\n",
    "\n",
    "        for row in chunk.iter_rows(named=True):\n",
    "            for table_name, column_mapping in table_mappings.items():\n",
    "                all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                mapped_columns = []\n",
    "                mapped_values = []\n",
    "\n",
    "                # Add mapped columns from the DataFrame\n",
    "                for table_col in all_columns:\n",
    "                    if table_col == 'AddressID':\n",
    "                        mapped_columns.append(table_col)\n",
    "                        mapped_values.append(f\"{address_id_counter}\")\n",
    "                    elif table_col in column_mapping:\n",
    "                        df_col = column_mapping[table_col]\n",
    "                        if df_col in row:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                    elif table_col not in foreign_key_columns:\n",
    "                        default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"' '\"})\n",
    "                        null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                        blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                        zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "\n",
    "                        if table_col in null_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"NULL\")\n",
    "                        elif table_col in blank_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"' '\")\n",
    "                        elif table_col in zero_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"0\")\n",
    "                        else:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                # Increment the AddressID counter\n",
    "                if 'AddressID' in mapped_columns:\n",
    "                    address_id_counter += 1\n",
    "\n",
    "                # Ensure the number of columns matches the number of values\n",
    "                if len(mapped_columns) == len(mapped_values):\n",
    "                    sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                    sql_conn.execute(sql_command)\n",
    "                else:\n",
    "                    print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "# Close the SQL connection after everything is completed\n",
    "sql_conn.close()\n",
    "\n",
    "print(\"Data population completed in Address table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = [f\"_{i}\" for i in range(1, 96)]\n",
    "# print(names)\n",
    "# created_databases=names\n",
    "# locations_per_split=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating database: RMS_EDM_THFL_1\n",
      "All rows deleted from Property table in database RMS_EDM_THFL_1.\n",
      "Data population completed in Property table.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import polars as pl\n",
    "\n",
    "# Class to manage SQL connection\n",
    "class SQLConnection:\n",
    "    def __init__(self, server, database):\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def open(self):\n",
    "        # Open a persistent connection\n",
    "        self.connection = pyodbc.connect(\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={self.server};\"\n",
    "            f\"DATABASE={self.database};\"\n",
    "            \"Trusted_Connection=yes;\"\n",
    "        )\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def execute(self, sql_command):\n",
    "        try:\n",
    "            #print(f\"Executing SQL Command: {sql_command}\")\n",
    "            self.cursor.execute(sql_command)\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing command: {sql_command}\\nException: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "\n",
    "# Function to get table columns\n",
    "def get_table_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to get foreign key columns\n",
    "def get_foreign_key_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"\"\"\n",
    "        SELECT COLUMN_NAME \n",
    "        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "        WHERE TABLE_NAME = '{table_name}' AND CONSTRAINT_NAME IN (\n",
    "            SELECT CONSTRAINT_NAME \n",
    "            FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching foreign key columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Server details\n",
    "server = \"localhost\"\n",
    "\n",
    "table_mappings = {\n",
    "    \"property\": {\"LOCNUM\":\"LOCNUM\",\"LOCNAME\":\"LOCNUM\",\n",
    "                 \"BLDGSCHEME\":\"BLDGSCHEME\",\"OCCSCHEME\":\"OCCSCHEME\",\"BLDGSCHEME\" :\"BLDGSCHEME\",\n",
    "                 \"OCCTYPE\":\"OCCTYPE\",\n",
    "                 \"OCCTYPE\":\"OCCTYPE\",\"NUMBLDGS\":\"NUMBLDGS\",\"NUMSTORIES\":\"NUMSTORIES\"},\n",
    "    \n",
    "}\n",
    "\n",
    "# Define behavior for unspecified columns\n",
    "unspecified_column_behavior = {\n",
    "    \"property\": {\n",
    "        \"default\": \"0\",  # General default value for unspecified columns\n",
    "        \"null_columns\": [],  # Columns where value should be NULL\n",
    "        \"blank_columns\": ['USERID1', 'USERID2', 'USERTXT1', 'USERTXT2', 'SITENAME', 'FLOOROCCUPANCY','HUZONE'],  # Columns where value should be a blank space\n",
    "        \"zero_columns\": [],  # Columns where value should be 0\n",
    "        \"specific_defaults\": {  # Columns with specific default values\n",
    "            \"YEARBUILT\": undate,\n",
    "            \"INCEPTDATE\": Idate,\n",
    "            \"EXPIREDATE\": Edate,\n",
    "            \"FLOODDEFENSEELEVATION\": \"-999\",\n",
    "            \"AREAUNIT\": \"2\",\n",
    "            \"HEIGHTUNIT\": \"2\",\n",
    "            \"PRIMARYBLDG\": \"1\",\n",
    "            \"FLOODDEFENSEELEVATIONUNIT\": \"2\",\n",
    "            \"FLOODDEFHTABOVEGRND\": \"-999\",\n",
    "            \"USERGROUNDELEV\": \"-999\",\n",
    "            \"USERBFE\": \"-999\",\n",
    "            \"CREATEDATETIME\": undate,\n",
    "            \"UPDATEDATETIME\": undate,\n",
    "            \"ACCGRPID\": \"1\",\n",
    "            'OTHERZONE':currency,\n",
    "            \n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "# Counter for AddressID, LOCID, and PRIMARYLOCID\n",
    "address_id_counter = 4\n",
    "loc_id_counter = 4  # Initialize LOCID counter\n",
    "primary_id_counter = 4  # Initialize PRIMARYLOCID counter\n",
    "\n",
    "# Assume df and created_databases are defined elsewhere\n",
    "# Split the DataFrame into chunks\n",
    "chunks = [df[i:i + locations_per_split] for i in range(0, len(df), locations_per_split)]\n",
    "\n",
    "# Open the SQL connection once\n",
    "sql_conn = SQLConnection(server, created_databases[0])\n",
    "sql_conn.open()\n",
    "\n",
    "# Populate each chunk into the corresponding database\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i < len(created_databases):\n",
    "        database = created_databases[i]\n",
    "        print(f\"Populating database: {database}\")\n",
    "\n",
    "        # Switch database if necessary\n",
    "        if sql_conn.database != database:\n",
    "            sql_conn.close()\n",
    "            sql_conn = SQLConnection(server, database)\n",
    "            sql_conn.open()\n",
    "\n",
    "        # Delete existing rows from the Property table\n",
    "        sql_conn.execute(\"DELETE FROM Property \")\n",
    "        print(f\"All rows deleted from Property table in database {database}.\")\n",
    "\n",
    "        for row in chunk.iter_rows(named=True):\n",
    "            for table_name, column_mapping in table_mappings.items():\n",
    "                all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                mapped_columns = []\n",
    "                mapped_values = []\n",
    "\n",
    "                # Add mapped columns from the DataFrame\n",
    "                for table_col in all_columns:\n",
    "                    if table_col == 'AddressID':\n",
    "                        mapped_columns.append(table_col)\n",
    "                        mapped_values.append(f\"{address_id_counter}\")\n",
    "                    elif table_col == 'LOCID':\n",
    "                        mapped_columns.append(table_col)\n",
    "                        mapped_values.append(f\"{loc_id_counter}\")\n",
    "                    elif table_col == 'PRIMARYLOCID':\n",
    "                        mapped_columns.append(table_col)\n",
    "                        mapped_values.append(f\"{primary_id_counter}\")\n",
    "                    elif table_col in column_mapping:\n",
    "                        df_col = column_mapping[table_col]\n",
    "                        if df_col in row:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                    elif table_col not in foreign_key_columns:\n",
    "                        default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"0\"})\n",
    "                        null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                        blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                        zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "                        specific_defaults = default_behavior.get(\"specific_defaults\", {})\n",
    "\n",
    "                        if table_col in null_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"NULL\")\n",
    "                        elif table_col in blank_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"' '\")\n",
    "                        elif table_col in zero_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"0\")\n",
    "                        elif table_col in specific_defaults:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"'{specific_defaults[table_col]}'\")\n",
    "                        else:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                # Increment the AddressID, LOCID, and PRIMARYLOCID counters\n",
    "                if 'AddressID' in mapped_columns:\n",
    "                    address_id_counter += 1\n",
    "                if 'LOCID' in mapped_columns:\n",
    "                    loc_id_counter += 1\n",
    "                if 'PRIMARYLOCID' in mapped_columns:\n",
    "                    primary_id_counter += 1\n",
    "\n",
    "                # Ensure the number of columns matches the number of values\n",
    "                if len(mapped_columns) == len(mapped_values):\n",
    "                    sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                    sql_conn.execute(sql_command)\n",
    "                else:\n",
    "                    print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "# Close the SQL connection after everything is completed\n",
    "sql_conn.close()\n",
    "\n",
    "print(\"Data population completed in Property table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now loccvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating database: RMS_EDM_THFL_1\n",
      "All rows deleted from loccvg table in database RMS_EDM_THFL_1.\n",
      "Data population completed in loccvg table.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import polars as pl\n",
    "\n",
    "# Class to manage SQL connection\n",
    "class SQLConnection:\n",
    "    def __init__(self, server, database):\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def open(self):\n",
    "        # Open a persistent connection\n",
    "        self.connection = pyodbc.connect(\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={self.server};\"\n",
    "            f\"DATABASE={self.database};\"\n",
    "            \"Trusted_Connection=yes;\"\n",
    "        )\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def execute(self, sql_command):\n",
    "        try:\n",
    "            self.cursor.execute(sql_command)\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing command: {sql_command}\\nException: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "\n",
    "# Function to get table columns\n",
    "def get_table_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to get foreign key columns\n",
    "def get_foreign_key_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"\"\"\n",
    "        SELECT COLUMN_NAME \n",
    "        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "        WHERE TABLE_NAME = '{table_name}' AND CONSTRAINT_NAME IN (\n",
    "            SELECT CONSTRAINT_NAME \n",
    "            FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching foreign key columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Server details\n",
    "server = \"localhost\"\n",
    "\n",
    "table_mappings = {\n",
    "    \"loccvg\": {\"VALUEAMT\":\"Value\",\"LOSSTYPE\":\"losstype\",\"LABELID\":\"labelid\",\"LOCID\":\"LOCNUM\"},\n",
    "    \n",
    "\n",
    "}\n",
    "\n",
    "# Define behavior for unspecified columns\n",
    "unspecified_column_behavior = {\n",
    "    \"loccvg\": {\n",
    "        \"default\": \"0\",  # General default value for unspecified columns\n",
    "        \"null_columns\": [],  # Columns where value should be NULL\n",
    "        \"blank_columns\": [], # Columns where value should be a blank space\n",
    "        \"zero_columns\": [],  # Columns where value should be 0\n",
    "        \"specific_defaults\":  {\n",
    "        \"ISVALID\": \"1\",\n",
    "        \"PERIL\": perilno,\n",
    "        \"LIMITCUR\": currency,\n",
    "        \"DEDUCTCUR\": currency,\n",
    "        \"VALUECUR\": currency,\n",
    "        \"NONRANKINGDEDUCTCUR\": currency,\n",
    "        \"BIPOI\":12\n",
    "    }\n",
    "    },\n",
    "}\n",
    "\n",
    "# Counter for AddressID, LOCID, and PRIMARYLOCID\n",
    "loccvg_id_counter = 4\n",
    "#loc_id_counter_counter = 4\n",
    "\n",
    "\n",
    "# Assume df and created_databases are defined elsewhere\n",
    "# Split the DataFrame into chunks\n",
    "chunks = [unpivoted_df[i:i + (locations_per_split*3)] for i in range(0, len(unpivoted_df), (locations_per_split*3))]\n",
    "\n",
    "# Open the SQL connection once\n",
    "sql_conn = SQLConnection(server, created_databases[0])\n",
    "sql_conn.open()\n",
    "\n",
    "# Populate each chunk into the corresponding database\n",
    "for i, chunk in enumerate(chunks):\n",
    "    if i < len(created_databases):\n",
    "        database = created_databases[i]\n",
    "        print(f\"Populating database: {database}\")\n",
    "\n",
    "        # Switch database if necessary\n",
    "        if sql_conn.database != database:\n",
    "            sql_conn.close()\n",
    "            sql_conn = SQLConnection(server, database)\n",
    "            sql_conn.open()\n",
    "\n",
    "        # Delete existing rows from the Property table\n",
    "        sql_conn.execute(\"DELETE FROM loccvg \")\n",
    "        print(f\"All rows deleted from loccvg table in database {database}.\")\n",
    "\n",
    "        for row in chunk.iter_rows(named=True):\n",
    "            for table_name, column_mapping in table_mappings.items():\n",
    "                all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                mapped_columns = []\n",
    "                mapped_values = []\n",
    "\n",
    "                # Add mapped columns from the DataFrame\n",
    "                for table_col in all_columns:\n",
    "                    if table_col == 'LOCCVGID':\n",
    "                        mapped_columns.append(table_col)\n",
    "                        mapped_values.append(f\"{loccvg_id_counter}\")\n",
    "                    # elif table_col == 'LOCID':\n",
    "                    #     mapped_columns.append(table_col)\n",
    "                    #     mapped_values.append(f\"{loc_id_counter_counter}\")\n",
    "                    \n",
    "                    elif table_col in column_mapping:\n",
    "                        df_col = column_mapping[table_col]\n",
    "                        if df_col in row:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                    elif table_col not in foreign_key_columns:\n",
    "                        default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"0\"})\n",
    "                        null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                        blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                        zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "                        specific_defaults = default_behavior.get(\"specific_defaults\", {})\n",
    "\n",
    "                        if table_col in null_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"NULL\")\n",
    "                        elif table_col in blank_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"' '\")\n",
    "                        elif table_col in zero_columns:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(\"0\")\n",
    "                        elif table_col in specific_defaults:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"'{specific_defaults[table_col]}'\")\n",
    "                        else:\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                # Increment the loccvg id counter counters\n",
    "                if 'LOCCVGID' in mapped_columns:\n",
    "                    loccvg_id_counter += 1\n",
    "                # if 'LOCID' in mapped_columns:\n",
    "                #     loc_id_counter_counter += 1\n",
    "                \n",
    "                \n",
    "\n",
    "                # Ensure the number of columns matches the number of values\n",
    "                if len(mapped_columns) == len(mapped_values):\n",
    "                    sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                    sql_conn.execute(sql_command)\n",
    "                else:\n",
    "                    print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "# Close the SQL connection after everything is completed\n",
    "sql_conn.close()\n",
    "\n",
    "print(\"Data population completed in loccvg table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"INSERT INTO loccvg (LOCCVGID, LOCID, PERIL, LIMITAMT, LIMITCUR, DEDUCTAMT, DEDUCTCUR, VALUEAMT, VALUECUR, LOSSTYPE, LABELID, COVGMOD, ACCUMID, EQSLGRADE, PCTSPRNKLR, WAITINGPERIOD, BIPOI, ISVALID, PCNTLOSSDEDAMT, ISFRANCHDED) VALUES (21, '9', '4', 0, 'USD', 0, 'USD', '10000000', 'USD', '3', '9', 0, 0, 0, 0, 0, '12', '1', 0, 0)\""
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sql_command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating database: RMS_EDM_THFL_1\n",
      "All rows deleted from fldet table in database RMS_EDM_THFL_1.\n",
      "Data population completed in fldet table.\n"
     ]
    }
   ],
   "source": [
    "import pyodbc\n",
    "import polars as pl\n",
    "\n",
    "# Class to manage SQL connection\n",
    "class SQLConnection:\n",
    "    def __init__(self, server, database):\n",
    "        self.server = server\n",
    "        self.database = database\n",
    "        self.connection = None\n",
    "        self.cursor = None\n",
    "\n",
    "    def open(self):\n",
    "        # Open a persistent connection\n",
    "        self.connection = pyodbc.connect(\n",
    "            f\"DRIVER={{ODBC Driver 17 for SQL Server}};\"\n",
    "            f\"SERVER={self.server};\"\n",
    "            f\"DATABASE={self.database};\"\n",
    "            \"Trusted_Connection=yes;\"\n",
    "        )\n",
    "        self.cursor = self.connection.cursor()\n",
    "\n",
    "    def execute(self, sql_command):\n",
    "        try:\n",
    "            self.cursor.execute(sql_command)\n",
    "            self.connection.commit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing command: {sql_command}\\nException: {e}\")\n",
    "\n",
    "    def close(self):\n",
    "        if self.cursor:\n",
    "            self.cursor.close()\n",
    "        if self.connection:\n",
    "            self.connection.close()\n",
    "\n",
    "# Function to get table columns\n",
    "def get_table_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"SELECT COLUMN_NAME FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{table_name}'\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Function to get foreign key columns\n",
    "def get_foreign_key_columns(cursor, table_name):\n",
    "    try:\n",
    "        sql_command = f\"\"\"\n",
    "        SELECT COLUMN_NAME \n",
    "        FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE \n",
    "        WHERE TABLE_NAME = '{table_name}' AND CONSTRAINT_NAME IN (\n",
    "            SELECT CONSTRAINT_NAME \n",
    "            FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS\n",
    "        )\n",
    "        \"\"\"\n",
    "        cursor.execute(sql_command)\n",
    "        columns = [row[0] for row in cursor.fetchall()]\n",
    "        return columns\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching foreign key columns for table {table_name}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Server details\n",
    "server = \"localhost\"\n",
    "\n",
    "\n",
    "\n",
    "if perilno==1:\n",
    "\n",
    "    table_mappings = {\n",
    "        \"eqdet\": {\"EQDETID\":\"LOCNUM\",\"LOCID\": \"LOCNUM\"}\n",
    "    }\n",
    "\n",
    "    # Define behavior for unspecified columns\n",
    "    unspecified_column_behavior = {\n",
    "        \"eqdet\": {\n",
    "            \"default\": \"0\",  # General default value for unspecified columns\n",
    "            \"null_columns\": [],  # Columns where value should be NULL\n",
    "            \"blank_columns\": ['MMI_VERSION', \"RMSCLASS\", \"ISOCLASS\", \"ATCCLASS\", \"FIRECLASS\", \"USERCLASS\",\n",
    "                            \"ATCOCC\", \"SICOCC\", \"ISOOCC\", \"IBCOCC\", \"USEROCC\"],  # Columns where value should be a blank space\n",
    "            \"zero_columns\": [],  # Columns where value should be 0\n",
    "            \"specific_defaults\": {  # Columns with specific default values\n",
    "                \"PCNTCOMPLT\": \"100\", \"NONRANKINGSITEDEDCUR\": currency, \"NONRANKINGCOMBINEDDEDCUR\": currency, \"SITEDEDCUR\": currency, \"ISVALID\": \"1\", \"DI\": \"-1\", \"STARTDATE\": undate, \"YEARUPGRAD\": undate, \"YEARSPNKLR\": undate, \"COMPDATE\": undate,\n",
    "                \"COMBINEDLIMCUR\": currency, \"COMBINEDDEDCUR\": currency, \"SITELIMCUR\": currency,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Counter for AddressID, LOCID, and PRIMARYLOCID\n",
    "    eqdet_counter = 4\n",
    "    loc_id_counter = 4  # Initialize LOCID counter\n",
    "\n",
    "    # Assume df and created_databases are defined elsewhere\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df[i:i + locations_per_split] for i in range(0, len(df), locations_per_split)]\n",
    "\n",
    "    # Open the SQL connection once\n",
    "    sql_conn = SQLConnection(server, created_databases[0])\n",
    "    sql_conn.open()\n",
    "\n",
    "    # Populate each chunk into the corresponding database\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i < len(created_databases):\n",
    "            database = created_databases[i]\n",
    "            print(f\"Populating database: {database}\")\n",
    "\n",
    "            # Switch database if necessary\n",
    "            if sql_conn.database != database:\n",
    "                sql_conn.close()\n",
    "                sql_conn = SQLConnection(server, database)\n",
    "                sql_conn.open()\n",
    "\n",
    "            # Delete existing rows from the Property table\n",
    "            sql_conn.execute(\"DELETE FROM eqdet \")\n",
    "            print(f\"All rows deleted from eqdet table in database {database}.\")\n",
    "\n",
    "            for row in chunk.iter_rows(named=True):\n",
    "                for table_name, column_mapping in table_mappings.items():\n",
    "                    all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                    foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                    mapped_columns = []\n",
    "                    mapped_values = []\n",
    "\n",
    "                    # Add mapped columns from the DataFrame\n",
    "                    for table_col in all_columns:\n",
    "                        if table_col == 'EQDETID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{eqdet_counter}\")\n",
    "                        elif table_col == 'LOCID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{loc_id_counter}\")\n",
    "                        \n",
    "                        elif table_col in column_mapping:\n",
    "                            df_col = column_mapping[table_col]\n",
    "                            if df_col in row:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                        elif table_col not in foreign_key_columns:\n",
    "                            default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"0\"})\n",
    "                            null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                            blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                            zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "                            specific_defaults = default_behavior.get(\"specific_defaults\", {})\n",
    "\n",
    "                            if table_col in null_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"NULL\")\n",
    "                            elif table_col in blank_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"' '\")\n",
    "                            elif table_col in zero_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"0\")\n",
    "                            elif table_col in specific_defaults:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{specific_defaults[table_col]}'\")\n",
    "                            else:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                    # Increment the AddressID, LOCID, and PRIMARYLOCID counters\n",
    "                    if 'EQDETID' in mapped_columns:\n",
    "                        eqdet_counter += 1\n",
    "                    if 'LOCID' in mapped_columns:\n",
    "                        loc_id_counter += 1\n",
    "                    \n",
    "\n",
    "                    # Ensure the number of columns matches the number of values\n",
    "                    if len(mapped_columns) == len(mapped_values):\n",
    "                        sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                        sql_conn.execute(sql_command)\n",
    "                    else:\n",
    "                        print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "    # Close the SQL connection after everything is completed\n",
    "    sql_conn.close()\n",
    "\n",
    "    print(\"Data population completed in eqdet table.\")\n",
    "\n",
    "elif perilno==2:\n",
    "\n",
    "    table_mappings = {\n",
    "        \"hudet\": {\"HUDETID\":\"LOCNUM\",\"LOCID\": \"LOCNUM\"}\n",
    "    }\n",
    "\n",
    "    # Define behavior for unspecified columns\n",
    "    unspecified_column_behavior = {\n",
    "        \"hudet\": {\n",
    "            \"default\": \"0\",  # General default value for unspecified columns\n",
    "            \"null_columns\": ['HUFLOORTYPE'],  # Columns where value should be NULL\n",
    "            \"blank_columns\": ['MMI_VERSION', \"RMSCLASS\", \"ISOCLASS\", \"ATCCLASS\", \"FIRECLASS\", \"USERCLASS\",\n",
    "                            \"ATCOCC\", \"SICOCC\", \"ISOOCC\", \"IBCOCC\", \"USEROCC\"],  # Columns where value should be a blank space\n",
    "            \"zero_columns\": [],  # Columns where value should be 0\n",
    "            \"specific_defaults\": {  # Columns with specific default values\n",
    "                \"BUILDINGELEVATION\": -999, 'RMSBUILDINGELEVATION':-999,'SITELIMCUR':currency,'SITEDEDCUR':currency, 'COMBINEDLIMCUR':currency, 'COMBINEDDEDCUR':currency,\n",
    "                'NFIPYEAR' :9999, 'HUZONEGROUP':-99,\"PCNTCOMPLT\":100, \"YEARUPGRAD\":undate, \"STARTDATE\":undate ,\"COMPDATE\":undate,        },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    hudet_counter = 4\n",
    "    loc_id_counter = 4  \n",
    "\n",
    "    chunks = [df[i:i + locations_per_split] for i in range(0, len(df), locations_per_split)]\n",
    "\n",
    "    # Open the SQL connection once\n",
    "    sql_conn = SQLConnection(server, created_databases[0])\n",
    "    sql_conn.open()\n",
    "\n",
    "    # Populate each chunk into the corresponding database\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i < len(created_databases):\n",
    "            database = created_databases[i]\n",
    "            print(f\"Populating database: {database}\")\n",
    "\n",
    "            # Switch database if necessary\n",
    "            if sql_conn.database != database:\n",
    "                sql_conn.close()\n",
    "                sql_conn = SQLConnection(server, database)\n",
    "                sql_conn.open()\n",
    "\n",
    "            # Delete existing rows from the Property table\n",
    "            sql_conn.execute(\"DELETE FROM hudet \")\n",
    "            print(f\"All rows deleted from eqdet table in database {database}.\")\n",
    "\n",
    "            for row in chunk.iter_rows(named=True):\n",
    "                for table_name, column_mapping in table_mappings.items():\n",
    "                    all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                    foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                    mapped_columns = []\n",
    "                    mapped_values = []\n",
    "\n",
    "                    # Add mapped columns from the DataFrame\n",
    "                    for table_col in all_columns:\n",
    "                        if table_col == 'HUDETID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{hudet_counter}\")\n",
    "                        elif table_col == 'LOCID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{loc_id_counter}\")\n",
    "                        \n",
    "                        elif table_col in column_mapping:\n",
    "                            df_col = column_mapping[table_col]\n",
    "                            if df_col in row:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                        elif table_col not in foreign_key_columns:\n",
    "                            default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"0\"})\n",
    "                            null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                            blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                            zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "                            specific_defaults = default_behavior.get(\"specific_defaults\", {})\n",
    "\n",
    "                            if table_col in null_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"NULL\")\n",
    "                            elif table_col in blank_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"' '\")\n",
    "                            elif table_col in zero_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"0\")\n",
    "                            elif table_col in specific_defaults:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{specific_defaults[table_col]}'\")\n",
    "                            else:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                    # Increment the AddressID, LOCID, and PRIMARYLOCID counters\n",
    "                    if 'HUDETID' in mapped_columns:\n",
    "                        hudet_counter += 1\n",
    "                    if 'LOCID' in mapped_columns:\n",
    "                        loc_id_counter += 1\n",
    "                    \n",
    "\n",
    "                    # Ensure the number of columns matches the number of values\n",
    "                    if len(mapped_columns) == len(mapped_values):\n",
    "                        sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                        sql_conn.execute(sql_command)\n",
    "                    else:\n",
    "                        print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "    # Close the SQL connection after everything is completed\n",
    "    sql_conn.close()\n",
    "\n",
    "    print(\"Data population completed in hudet table.\")\n",
    "\n",
    "elif perilno==3:\n",
    "\n",
    "    table_mappings = {\n",
    "        \"todet\": {\"TODETID\":\"LOCNUM\",\"LOCID\": \"LOCNUM\"}\n",
    "    }\n",
    "\n",
    "    # Define behavior for unspecified columns\n",
    "    unspecified_column_behavior = {\n",
    "        \"todet\": {\n",
    "            \"default\": \"0\",  # General default value for unspecified columns\n",
    "            \"null_columns\": [],  # Columns where value should be NULL\n",
    "            \"blank_columns\": ['RMSCLASS', 'FIRECLASS', 'USERCLASS', 'ATCOCC', 'ISOOCC', 'USEROCC'],  # Columns where value should be a blank space\n",
    "            \"zero_columns\": [],  # Columns where value should be 0\n",
    "            \"specific_defaults\": {  # Columns with specific default values\n",
    "                \"PCNTCOMPLT\": \"100\",  \"SITEDEDCUR\": currency, \"ISVALID\": \"1\", \"YEARUPGRAD\": undate, \"YEARSPNKLR\": undate,\n",
    "                \"COMBINEDLIMCUR\": currency, \"COMBINEDDEDCUR\": currency, \"SITELIMCUR\": currency,\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Counter for AddressID, LOCID, and PRIMARYLOCID\n",
    "    todet_counter = 4\n",
    "    loc_id_counter = 4  # Initialize LOCID counter\n",
    "\n",
    "    # Assume df and created_databases are defined elsewhere\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df[i:i + locations_per_split] for i in range(0, len(df), locations_per_split)]\n",
    "\n",
    "    # Open the SQL connection once\n",
    "    sql_conn = SQLConnection(server, created_databases[0])\n",
    "    sql_conn.open()\n",
    "\n",
    "    # Populate each chunk into the corresponding database\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i < len(created_databases):\n",
    "            database = created_databases[i]\n",
    "            print(f\"Populating database: {database}\")\n",
    "\n",
    "            # Switch database if necessary\n",
    "            if sql_conn.database != database:\n",
    "                sql_conn.close()\n",
    "                sql_conn = SQLConnection(server, database)\n",
    "                sql_conn.open()\n",
    "\n",
    "            # Delete existing rows from the Property table\n",
    "            sql_conn.execute(\"DELETE FROM todet \")\n",
    "            print(f\"All rows deleted from eqdet table in database {database}.\")\n",
    "\n",
    "            for row in chunk.iter_rows(named=True):\n",
    "                for table_name, column_mapping in table_mappings.items():\n",
    "                    all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                    foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                    mapped_columns = []\n",
    "                    mapped_values = []\n",
    "\n",
    "                    # Add mapped columns from the DataFrame\n",
    "                    for table_col in all_columns:\n",
    "                        if table_col == 'TODETID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{todet_counter}\")\n",
    "                        elif table_col == 'LOCID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{loc_id_counter}\")\n",
    "                        \n",
    "                        elif table_col in column_mapping:\n",
    "                            df_col = column_mapping[table_col]\n",
    "                            if df_col in row:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                        elif table_col not in foreign_key_columns:\n",
    "                            default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"0\"})\n",
    "                            null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                            blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                            zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "                            specific_defaults = default_behavior.get(\"specific_defaults\", {})\n",
    "\n",
    "                            if table_col in null_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"NULL\")\n",
    "                            elif table_col in blank_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"' '\")\n",
    "                            elif table_col in zero_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"0\")\n",
    "                            elif table_col in specific_defaults:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{specific_defaults[table_col]}'\")\n",
    "                            else:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                    # Increment the AddressID, LOCID, and PRIMARYLOCID counters\n",
    "                    if 'TODETID' in mapped_columns:\n",
    "                        todet_counter += 1\n",
    "                    if 'LOCID' in mapped_columns:\n",
    "                        loc_id_counter += 1\n",
    "                    \n",
    "\n",
    "                    # Ensure the number of columns matches the number of values\n",
    "                    if len(mapped_columns) == len(mapped_values):\n",
    "                        sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                        sql_conn.execute(sql_command)\n",
    "                    else:\n",
    "                        print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "    # Close the SQL connection after everything is completed\n",
    "    sql_conn.close()\n",
    "\n",
    "    print(\"Data population completed in todet table.\")\n",
    "\n",
    "    \n",
    "elif perilno == 4:\n",
    "\n",
    "    table_mappings = {\n",
    "        \"fldet\": {\"FLDETID\": \"LOCNUM\", \"LOCID\": \"LOCNUM\"}\n",
    "    }\n",
    "\n",
    "    # Define behavior for unspecified columns\n",
    "    unspecified_column_behavior = {\n",
    "        \"fldet\": {\n",
    "            \"default\": \"0\",  # General default value for unspecified columns\n",
    "            \"null_columns\": [],  # Columns where value should be NULL\n",
    "            \"blank_columns\": ['BFE', 'ADDLINFO', 'PANEL', 'COBRA', 'FLOODWAY', 'SFHA', 'COMMUNITY', 'UNDERREV', 'OTHERZONES', 'USERID1', 'USERID2', 'ANNPROB', 'BASINNAME', 'FLOODDRIVER', 'LEVEES'],  # Columns where value should be a blank space\n",
    "            \"zero_columns\": [],  # Columns where value should be 0\n",
    "            \"specific_defaults\": {  # Columns with specific default values\n",
    "                \"PCNTCOMPLT\": \"100\", \"SITEDEDCUR\": currency, \"ISVALID\": \"1\", \"RMS100FLZONE\": \"-1\", \"RMS500FLZONE\": \"-1\", \"STARTDATE\": undate, \"YEARUPGRAD\": undate, \"COMPDATE\": undate, \"PANELDATE\": undate,\n",
    "                \"COMBINEDLIMCUR\": currency, \"COMBINEDDEDCUR\": currency, \"SITELIMCUR\": currency, 'FINISHEDFLOOR': \"-999\", 'FLFFHAG': \"-999\", 'FLZONEGROUP': \"-99\",\n",
    "                '\"50YRRPDEF\"': \"0\", '\"75YRRPDEF\"': \"0\", '\"100YRRPDEF\"': \"0\", '\"200YRRPDEF\"': \"0\", '\"250YRRPDEF\"': \"0\", '\"500YRRPDEF\"': \"0\", '\"1000YRRPDEF\"': \"0\", '\"10000YRRPDEF\"': \"0\",\n",
    "                '\"50YRRPUNDEF\"': \"0\", '\"75YRRPUNDEF\"': \"0\", '\"100YRRPUNDEF\"': \"0\", '\"200YRRPUNDEF\"': \"0\", '\"250YRRPUNDEF\"': \"0\", '\"500YRRPUNDEF\"': \"0\", '\"1000YRRPUNDEF\"': \"0\", '\"10000YRRPUNDEF\"': \"0\",\n",
    "                '\"30YRRPDEF\"': \"0\", '\"30YRRPUNDEF\"': \"0\"\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Counter for AddressID, LOCID, and PRIMARYLOCID\n",
    "    fldet_counter = 4\n",
    "    loc_id_counter = 4  # Initialize LOCID counter\n",
    "\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df[i:i + locations_per_split] for i in range(0, len(df), locations_per_split)]\n",
    "\n",
    "    # Open the SQL connection once\n",
    "    sql_conn = SQLConnection(server, created_databases[0])\n",
    "    sql_conn.open()\n",
    "\n",
    "    # Helper function to quote column names starting with a number\n",
    "    def quote_column_name(column_name):\n",
    "        if column_name[0].isdigit():\n",
    "            return f'\"{column_name}\"'\n",
    "        return column_name\n",
    "\n",
    "    # Populate each chunk into the corresponding database\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i < len(created_databases):\n",
    "            database = created_databases[i]\n",
    "            print(f\"Populating database: {database}\")\n",
    "\n",
    "            # Switch database if necessary\n",
    "            if sql_conn.database != database:\n",
    "                sql_conn.close()\n",
    "                sql_conn = SQLConnection(server, database)\n",
    "                sql_conn.open()\n",
    "\n",
    "            # Delete existing rows from the Property table\n",
    "            sql_conn.execute(\"DELETE FROM fldet \")\n",
    "            print(f\"All rows deleted from fldet table in database {database}.\")\n",
    "\n",
    "            for row in chunk.iter_rows(named=True):\n",
    "                for table_name, column_mapping in table_mappings.items():\n",
    "                    all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                    foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                    mapped_columns = []\n",
    "                    mapped_values = []\n",
    "\n",
    "                    # Add mapped columns from the DataFrame\n",
    "                    for table_col in all_columns:\n",
    "                        if table_col == 'FLDETID':\n",
    "                            mapped_columns.append(quote_column_name(table_col))\n",
    "                            mapped_values.append(f\"{fldet_counter}\")\n",
    "                        elif table_col == 'LOCID':\n",
    "                            mapped_columns.append(quote_column_name(table_col))\n",
    "                            mapped_values.append(f\"{loc_id_counter}\")\n",
    "                        \n",
    "                        elif table_col in column_mapping:\n",
    "                            df_col = column_mapping[table_col]\n",
    "                            if df_col in row:\n",
    "                                mapped_columns.append(quote_column_name(table_col))\n",
    "                                mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                        elif table_col not in foreign_key_columns:\n",
    "                            default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"0\"})\n",
    "                            null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                            blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                            zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "                            specific_defaults = default_behavior.get(\"specific_defaults\", {})\n",
    "\n",
    "                            if table_col in null_columns:\n",
    "                                mapped_columns.append(quote_column_name(table_col))\n",
    "                                mapped_values.append(\"NULL\")\n",
    "                            elif table_col in blank_columns:\n",
    "                                mapped_columns.append(quote_column_name(table_col))\n",
    "                                mapped_values.append(\"' '\")\n",
    "                            elif table_col in zero_columns:\n",
    "                                mapped_columns.append(quote_column_name(table_col))\n",
    "                                mapped_values.append(\"0\")\n",
    "                            elif table_col in specific_defaults:\n",
    "                                mapped_columns.append(quote_column_name(table_col))\n",
    "                                mapped_values.append(f\"'{specific_defaults[table_col]}'\")\n",
    "                            else:\n",
    "                                mapped_columns.append(quote_column_name(table_col))\n",
    "                                mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                    # Increment the AddressID, LOCID, and PRIMARYLOCID counters\n",
    "                    if 'FLDETID' in mapped_columns:\n",
    "                        fldet_counter += 1\n",
    "                    if 'LOCID' in mapped_columns:\n",
    "                        loc_id_counter += 1\n",
    "                    \n",
    "\n",
    "                    # Ensure the number of columns matches the number of values\n",
    "                    if len(mapped_columns) == len(mapped_values):\n",
    "                        sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                        sql_conn.execute(sql_command)\n",
    "                    else:\n",
    "                        print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "    # Close the SQL connection after everything is completed\n",
    "    sql_conn.close()\n",
    "\n",
    "    print(\"Data population completed in fldet table.\")\n",
    "\n",
    "elif perilno==5:\n",
    "\n",
    "    table_mappings = {\n",
    "        \"frdet\": {\"FRDETID\":\"LOCNUM\",\"LOCID\": \"LOCNUM\"}\n",
    "    }\n",
    "\n",
    "    # Define behavior for unspecified columns\n",
    "    unspecified_column_behavior = {\n",
    "        \"frdet\": {\n",
    "            \"default\": \"0\",  # General default value for unspecified columns\n",
    "            \"null_columns\": [],  # Columns where value should be NULL\n",
    "            \"blank_columns\": ['WFSURFFUEL', 'WFSPECCOND', 'WFSITEHAZVERSION'],  # Columns where value should be a blank space\n",
    "            \"zero_columns\": [],  # Columns where value should be 0\n",
    "            \"specific_defaults\": {  # Columns with specific default values\n",
    "                \"PCNTCOMPLT\": \"100\",  \"NONRANKINGCOMBINEDDEDCUR\": currency, \"SITEDEDCUR\": currency, \"ISVALID\": \"1\", \"DI\": \"-1\", \"STARTDATE\": undate, \"YEARUPGRAD\": undate, \"COMPDATE\": undate,\n",
    "                \"COMBINEDLIMCUR\": currency, \"COMBINEDDEDCUR\": currency, \"SITELIMCUR\": currency,'WFSLOPE':-999, 'WFD2V':-999, 'DISTANCETOCALFIREHIGH':-999, 'DISTANCETOCALFIREMEDIUM':-999, 'DISTANCETOCALFIREVERYHIGH':-999,'YEARLASTHISTFIRE250M':9999,\n",
    "\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "\n",
    "    # Counter for AddressID, LOCID, and PRIMARYLOCID\n",
    "    frdet_counter = 4\n",
    "    loc_id_counter = 4  # Initialize LOCID counter\n",
    "\n",
    "    # Assume df and created_databases are defined elsewhere\n",
    "    # Split the DataFrame into chunks\n",
    "    chunks = [df[i:i + locations_per_split] for i in range(0, len(df), locations_per_split)]\n",
    "\n",
    "    # Open the SQL connection once\n",
    "    sql_conn = SQLConnection(server, created_databases[0])\n",
    "    sql_conn.open()\n",
    "\n",
    "    # Populate each chunk into the corresponding database\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        if i < len(created_databases):\n",
    "            database = created_databases[i]\n",
    "            print(f\"Populating database: {database}\")\n",
    "\n",
    "            # Switch database if necessary\n",
    "            if sql_conn.database != database:\n",
    "                sql_conn.close()\n",
    "                sql_conn = SQLConnection(server, database)\n",
    "                sql_conn.open()\n",
    "\n",
    "            # Delete existing rows from the Property table\n",
    "            sql_conn.execute(\"DELETE FROM frdet \")\n",
    "            print(f\"All rows deleted from frdet table in database {database}.\")\n",
    "\n",
    "            for row in chunk.iter_rows(named=True):\n",
    "                for table_name, column_mapping in table_mappings.items():\n",
    "                    all_columns = get_table_columns(sql_conn.cursor, table_name)\n",
    "                    foreign_key_columns = get_foreign_key_columns(sql_conn.cursor, table_name)\n",
    "\n",
    "                    mapped_columns = []\n",
    "                    mapped_values = []\n",
    "\n",
    "                    # Add mapped columns from the DataFrame\n",
    "                    for table_col in all_columns:\n",
    "                        if table_col == 'FRDETID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{frdet_counter}\")\n",
    "                        elif table_col == 'LOCID':\n",
    "                            mapped_columns.append(table_col)\n",
    "                            mapped_values.append(f\"{loc_id_counter}\")\n",
    "                        \n",
    "                        elif table_col in column_mapping:\n",
    "                            df_col = column_mapping[table_col]\n",
    "                            if df_col in row:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{row[df_col]}'\")\n",
    "                        elif table_col not in foreign_key_columns:\n",
    "                            default_behavior = unspecified_column_behavior.get(table_name, {\"default\": \"0\"})\n",
    "                            null_columns = default_behavior.get(\"null_columns\", [])\n",
    "                            blank_columns = default_behavior.get(\"blank_columns\", [])\n",
    "                            zero_columns = default_behavior.get(\"zero_columns\", [])\n",
    "                            specific_defaults = default_behavior.get(\"specific_defaults\", {})\n",
    "\n",
    "                            if table_col in null_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"NULL\")\n",
    "                            elif table_col in blank_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"' '\")\n",
    "                            elif table_col in zero_columns:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(\"0\")\n",
    "                            elif table_col in specific_defaults:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(f\"'{specific_defaults[table_col]}'\")\n",
    "                            else:\n",
    "                                mapped_columns.append(table_col)\n",
    "                                mapped_values.append(default_behavior.get(\"default\", \"0\"))\n",
    "\n",
    "                    # Increment the AddressID, LOCID, and PRIMARYLOCID counters\n",
    "                    if 'FRDETID' in mapped_columns:\n",
    "                        frdet_counter += 1\n",
    "                    if 'LOCID' in mapped_columns:\n",
    "                        loc_id_counter += 1\n",
    "                    \n",
    "\n",
    "                    # Ensure the number of columns matches the number of values\n",
    "                    if len(mapped_columns) == len(mapped_values):\n",
    "                        sql_command = f\"INSERT INTO {table_name} ({', '.join(mapped_columns)}) VALUES ({', '.join(mapped_values)})\"\n",
    "                        sql_conn.execute(sql_command)\n",
    "                    else:\n",
    "                        print(\"Mismatch in columns and values, skipping row.\")\n",
    "\n",
    "    # Close the SQL connection after everything is completed\n",
    "    sql_conn.close()\n",
    "\n",
    "    print(\"Data population completed in frdet table.\")\n",
    "\n",
    "\n",
    "\n",
    "else:\n",
    "    print(\"PERIL NOT VALID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
